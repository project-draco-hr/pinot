{
  Job job=Job.getInstance(getConf());
  job.setJarByClass(BackfillPhaseJob.class);
  job.setJobName(name);
  FileSystem fs=FileSystem.get(getConf());
  Configuration configuration=job.getConfiguration();
  LOGGER.info("*******************************************************************************");
  String controllerHost=getAndSetConfiguration(configuration,BACKFILL_PHASE_CONTROLLER_HOST);
  String controllerPort=getAndSetConfiguration(configuration,BACKFILL_PHASE_CONTROLLER_PORT);
  LOGGER.info("Controller Host : {} Controller Port : {}",controllerHost,controllerPort);
  String segmentStartTime=getAndSetConfiguration(configuration,BACKFILL_PHASE_START_TIME);
  String segmentEndTime=getAndSetConfiguration(configuration,BACKFILL_PHASE_END_TIME);
  long startTime=Long.valueOf(segmentStartTime);
  long endTime=Long.valueOf(segmentEndTime);
  if (Long.valueOf(segmentStartTime) > Long.valueOf(segmentEndTime)) {
    throw new IllegalStateException("Start time cannot be greater than end time");
  }
  String tableName=getAndSetConfiguration(configuration,BACKFILL_PHASE_TABLE_NAME);
  LOGGER.info("Start time : {} End time : {} Table name : {}",segmentStartTime,segmentEndTime,tableName);
  String outputPath=getAndSetConfiguration(configuration,BACKFILL_PHASE_OUTPUT_PATH);
  LOGGER.info("Output path : {}",outputPath);
  Path backfillDir=new Path(outputPath);
  if (fs.exists(backfillDir)) {
    LOGGER.warn("Found the output folder deleting it");
    fs.delete(backfillDir,true);
  }
  Path downloadDir=new Path(backfillDir,DOWNLOAD);
  LOGGER.info("Creating download dir : {}",downloadDir);
  fs.mkdirs(downloadDir);
  Path inputDir=new Path(backfillDir,INPUT);
  LOGGER.info("Creating input dir : {}",inputDir);
  fs.mkdirs(inputDir);
  Path outputDir=new Path(backfillDir,OUTPUT);
  LOGGER.info("Creating output dir : {}",outputDir);
  BackfillControllerAPIs backfillControllerAPIs=new BackfillControllerAPIs(controllerHost,Integer.valueOf(controllerPort),tableName);
  LOGGER.info("Downloading segments in range {} to {}",startTime,endTime);
  List<String> allSegments=backfillControllerAPIs.getAllSegments(tableName);
  List<String> segmentsToDownload=backfillControllerAPIs.findSegmentsInRange(tableName,allSegments,startTime,endTime);
  for (  String segmentName : segmentsToDownload) {
    backfillControllerAPIs.downloadSegment(segmentName,downloadDir);
  }
  LOGGER.info("Reading downloaded segment input files");
  List<FileStatus> inputDataFiles=new ArrayList<>();
  inputDataFiles.addAll(Lists.newArrayList(fs.listStatus(downloadDir)));
  LOGGER.info("size {}",inputDataFiles.size());
  try {
    LOGGER.info("Creating input files at {} for segment input files",inputDir);
    for (int seqId=0; seqId < inputDataFiles.size(); ++seqId) {
      FileStatus file=inputDataFiles.get(seqId);
      String completeFilePath=" " + file.getPath().toString() + " "+ seqId;
      Path newOutPutFile=new Path((inputDir + "/" + file.getPath().toString().replace('.','_').replace('/','_').replace(':','_')+ ".txt"));
      FSDataOutputStream stream=fs.create(newOutPutFile);
      LOGGER.info("wrote {}",completeFilePath);
      stream.writeUTF(completeFilePath);
      stream.flush();
      stream.close();
    }
  }
 catch (  Exception e) {
    LOGGER.error("Exception while reading input files ",e);
  }
  job.setMapperClass(BackfillPhaseMapJob.BackfillMapper.class);
  if (System.getenv("HADOOP_TOKEN_FILE_LOCATION") != null) {
    job.getConfiguration().set("mapreduce.job.credentials.binary",System.getenv("HADOOP_TOKEN_FILE_LOCATION"));
  }
  job.setInputFormatClass(TextInputFormat.class);
  job.setOutputFormatClass(TextOutputFormat.class);
  job.setMapOutputKeyClass(LongWritable.class);
  job.setMapOutputValueClass(Text.class);
  FileInputFormat.addInputPath(job,inputDir);
  FileOutputFormat.setOutputPath(job,outputDir);
  job.getConfiguration().setInt(JobContext.NUM_MAPS,inputDataFiles.size());
  job.setMaxReduceAttempts(1);
  job.setMaxMapAttempts(0);
  job.setNumReduceTasks(0);
  for (  Object key : props.keySet()) {
    job.getConfiguration().set(key.toString(),props.getProperty(key.toString()));
  }
  job.waitForCompletion(true);
  if (!job.isSuccessful()) {
    throw new RuntimeException("Job failed : " + job);
  }
  LOGGER.info("Cleanup the working directory");
  LOGGER.info("Deleting the dir: {}",downloadDir);
  fs.delete(downloadDir,true);
  LOGGER.info("Deleting the dir: {}",inputDir);
  fs.delete(inputDir,true);
  LOGGER.info("Deleting the dir: {}",outputDir);
  fs.delete(outputDir,true);
  return job;
}
