{
  _segmentZKMetadata=(LLCRealtimeSegmentZKMetadata)segmentZKMetadata;
  _tableConfig=tableConfig;
  _instanceZKMetadata=instanceZKMetadata;
  _realtimeTableDataManager=realtimeTableDataManager;
  _absolutePath=absolutePath;
  _readMode=readMode;
  _schema=schema;
  _serverMetrics=serverMetrics;
  final String bootstrapNodes=_tableConfig.getIndexingConfig().getStreamConfigs().get(CommonConstants.Helix.DataSource.STREAM_PREFIX + "." + CommonConstants.Helix.DataSource.Realtime.Kafka.KAFKA_BROKER_LIST);
  final String kafkaTopic=_tableConfig.getIndexingConfig().getStreamConfigs().get(CommonConstants.Helix.DataSource.STREAM_PREFIX + "." + CommonConstants.Helix.DataSource.Realtime.Kafka.TOPIC_NAME);
  final LLCSegmentName segmentName=new LLCSegmentName(_segmentZKMetadata.getSegmentName());
  final int kafkaPartitionId=segmentName.getPartitionId();
  final String decoderClassName=_tableConfig.getIndexingConfig().getStreamConfigs().get(CommonConstants.Helix.DataSource.STREAM_PREFIX + "." + CommonConstants.Helix.DataSource.Realtime.Kafka.DECODER_CLASS);
  final int segmentMaxRowCount=Integer.parseInt(_tableConfig.getIndexingConfig().getStreamConfigs().get(CommonConstants.Helix.DataSource.Realtime.REALTIME_SEGMENT_FLUSH_SIZE));
  _realtimeSegment=new RealtimeSegmentImpl(schema,segmentMaxRowCount,tableConfig.getTableName(),segmentZKMetadata.getSegmentName(),kafkaTopic,serverMetrics);
  _realtimeSegment.setSegmentMetadata(segmentZKMetadata,schema);
  final KafkaMessageDecoder messageDecoder=(KafkaMessageDecoder)Class.forName(decoderClassName).newInstance();
  messageDecoder.init(new HashMap<String,String>(),_schema,kafkaTopic);
  final PlainFieldExtractor fieldExtractor=(PlainFieldExtractor)FieldExtractorFactory.getPlainFieldExtractor(schema);
  _indexingThread=new Thread(new Runnable(){
    @Override public void run(){
      try {
        String clientId=kafkaPartitionId + "-" + NetUtil.getHostnameOrAddress();
        SimpleConsumerWrapper consumerWrapper=SimpleConsumerWrapper.forPartitionConsumption(new KafkaSimpleConsumerFactoryImpl(),bootstrapNodes,clientId,kafkaTopic,kafkaPartitionId);
        boolean notFull=true;
        long currentOffset=_segmentZKMetadata.getStartOffset();
        while (notFull) {
          final long endOffset=Long.MAX_VALUE;
          Iterable<MessageAndOffset> messagesAndOffsets=consumerWrapper.fetchMessages(currentOffset,endOffset,KAFKA_MAX_FETCH_TIME_MILLIS);
          int batchSize=0;
          for (          MessageAndOffset messageAndOffset : messagesAndOffsets) {
            byte[] array=messageAndOffset.message().payload().array();
            int offset=messageAndOffset.message().payload().arrayOffset();
            int length=messageAndOffset.message().payloadSize();
            GenericRow row=messageDecoder.decode(array,offset,length);
            if (row != null) {
              row=fieldExtractor.transform(row);
              notFull=_realtimeSegment.index(row);
              batchSize++;
            }
            currentOffset=messageAndOffset.nextOffset();
          }
          if (batchSize != 0) {
            LOGGER.debug("Indexed {} messages from partition {}, current offset {}",batchSize,kafkaPartitionId,currentOffset);
          }
 else {
            Uninterruptibles.sleepUninterruptibly(100,TimeUnit.MILLISECONDS);
          }
        }
      }
 catch (      Exception e) {
        LOGGER.error("Caught exception while indexing events",e);
      }
    }
  }
,"Realtime indexing thread for " + _segmentZKMetadata.getSegmentName());
  _indexingThread.start();
}
