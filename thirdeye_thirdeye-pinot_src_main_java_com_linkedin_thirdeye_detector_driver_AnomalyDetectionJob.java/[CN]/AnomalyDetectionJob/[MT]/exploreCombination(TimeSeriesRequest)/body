{
  LOG.info("Exploring {}",request);
  TimeSeriesResponse response;
  try {
    LOG.debug("Executing {}",request);
    response=timeSeriesHandler.handle(request);
  }
 catch (  Exception e) {
    throw new JobExecutionException(e);
  }
  Map<DimensionKey,MetricTimeSeries> res=timeSeriesResponseConverter.toMap(response,collectionDimensions);
  for (  Map.Entry<DimensionKey,MetricTimeSeries> entry : res.entrySet()) {
    if (entry.getValue().getTimeWindowSet().size() < 2) {
      LOG.warn("Insufficient data for {} to run anomaly detection function",entry.getKey());
      continue;
    }
    try {
      long startTime=System.currentTimeMillis();
      DimensionKey dimensionKey=entry.getKey();
      MetricTimeSeries metricTimeSeries=entry.getValue();
      LOG.info("Analyzing anomaly function with dimensionKey: {}, windowStart: {}, windowEnd: {}",dimensionKey,windowStart,windowEnd);
      List<AnomalyResult> results=anomalyFunction.analyze(dimensionKey,metricTimeSeries,windowStart,windowEnd,knownAnomalies);
      long endTime=System.currentTimeMillis();
      LOG.info("Updating histogram with startTime: {}, endTime: {}",startTime,endTime);
      histogram.update(endTime - startTime);
      handleResults(results);
      results.removeAll(knownAnomalies);
      LOG.info("{} has {} anomalies in window {} to {}",entry.getKey(),results.size(),windowStart,windowEnd);
      anomalyCounter+=results.size();
    }
 catch (    Exception e) {
      LOG.error("Could not compute for {}",entry.getKey(),e);
    }
  }
  return ImmutableList.of();
}
