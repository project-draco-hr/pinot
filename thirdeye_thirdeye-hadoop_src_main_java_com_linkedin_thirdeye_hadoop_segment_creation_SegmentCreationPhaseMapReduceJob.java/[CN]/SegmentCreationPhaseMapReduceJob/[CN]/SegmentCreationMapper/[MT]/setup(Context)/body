{
  currentHdfsWorkDir=FileOutputFormat.getWorkOutputPath(context);
  currentDiskWorkDir="pinot_hadoop_tmp";
  localHdfsSegmentTarPath=currentHdfsWorkDir + "/segmentTar";
  localDiskSegmentDirectory=currentDiskWorkDir + "/segments/";
  localDiskSegmentTarPath=currentDiskWorkDir + "/segmentsTar/";
  new File(localDiskSegmentTarPath).mkdirs();
  LOGGER.info("*********************************************************************");
  LOGGER.info("Configurations : {}",context.getConfiguration().toString());
  LOGGER.info("*********************************************************************");
  LOGGER.info("Current HDFS working dir : {}",currentHdfsWorkDir);
  LOGGER.info("Current DISK working dir : {}",new File(currentDiskWorkDir).getAbsolutePath());
  LOGGER.info("*********************************************************************");
  properties=context.getConfiguration();
  outputPath=properties.get(SEGMENT_CREATION_OUTPUT_PATH.toString());
  thirdeyeConfig=OBJECT_MAPPER.readValue(properties.get(SEGMENT_CREATION_THIRDEYE_CONFIG.toString()),ThirdEyeConfig.class);
  LOGGER.info(thirdeyeConfig.encode());
  schema=ThirdeyePinotSchemaUtils.createSchema(thirdeyeConfig);
  tableName=thirdeyeConfig.getCollection();
  segmentWallClockStartTime=Long.valueOf(properties.get(SEGMENT_CREATION_WALLCLOCK_START_TIME.toString()));
  segmentWallClockEndTime=Long.valueOf(properties.get(SEGMENT_CREATION_WALLCLOCK_END_TIME.toString()));
  segmentSchedule=properties.get(SEGMENT_CREATION_SCHEDULE.toString());
}
