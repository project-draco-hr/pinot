{
  Job job=Job.getInstance(getConf());
  Configuration conf=job.getConfiguration();
  job.setJobName(name);
  job.setJarByClass(JoinPhaseJob.class);
  FileSystem fs=FileSystem.get(conf);
  String outputSchemaPath=getAndSetConfiguration(conf,JOIN_OUTPUT_SCHEMA);
  Schema.Parser parser=new Schema.Parser();
  Schema outputSchema=parser.parse(fs.open(new Path(outputSchemaPath)));
  LOGGER.info("{}",outputSchema);
  String joinConfigUDFClass=getAndSetConfiguration(conf,JoinPhaseConstants.JOIN_CONFIG_UDF_CLASS);
  LOGGER.info("Initializing JoinConfigUDFClass:{} with params:{}",joinConfigUDFClass);
  Constructor<?> constructor=Class.forName(joinConfigUDFClass).getConstructor();
  JoinConfigUDF joinConfigUDF=(JoinConfigUDF)constructor.newInstance();
  joinConfigUDF.setJoinConfig(job);
  getAndSetConfiguration(conf,JOIN_KEY_EXTRACTOR_CLASS);
  getAndSetConfiguration(conf,JOIN_UDF_CLASS);
  List<String> sourceNames=Lists.newArrayList(getAndSetConfiguration(conf,JoinPhaseConstants.JOIN_SOURCE_NAMES).split(","));
  job.setMapperClass(GenericJoinMapper.class);
  job.setInputFormatClass(DelegatingAvroKeyInputFormat.class);
  job.setMapOutputKeyClass(BytesWritable.class);
  job.setMapOutputValueClass(BytesWritable.class);
  job.setReducerClass(GenericJoinReducer.class);
  AvroJob.setOutputKeySchema(job,outputSchema);
  job.setOutputFormatClass(AvroKeyOutputFormat.class);
  job.setOutputKeyClass(AvroKey.class);
  job.setOutputValueClass(NullWritable.class);
  String numReducers=props.getProperty("num.reducers");
  if (numReducers != null) {
    job.setNumReduceTasks(Integer.parseInt(numReducers));
  }
 else {
    job.setNumReduceTasks(10);
  }
  LOGGER.info("Setting number of reducers : " + job.getNumReduceTasks());
  Map<String,String> schemaMap=new HashMap<String,String>();
  Map<String,String> schemaPathMapping=new HashMap<String,String>();
  for (  String sourceName : sourceNames) {
    LOGGER.info("Loading Schema for {}",sourceName);
    FSDataInputStream schemaStream=fs.open(new Path(getAndCheck(sourceName + "." + JOIN_INPUT_SCHEMA.toString())));
    Schema schema=new Schema.Parser().parse(schemaStream);
    schemaMap.put(sourceName,schema.toString());
    LOGGER.info("Schema for {}:  \n{}",sourceName,schema);
    String inputPathDir=getAndCheck(sourceName + "." + JOIN_INPUT_PATH.toString());
    LOGGER.info("Input path dir for " + sourceName + ": "+ inputPathDir);
    for (    String inputPath : inputPathDir.split(",")) {
      Path input=new Path(inputPath);
      FileStatus[] listFiles=fs.listStatus(input);
      boolean isNested=false;
      for (      FileStatus fileStatus : listFiles) {
        if (fileStatus.isDirectory()) {
          isNested=true;
          Path path=fileStatus.getPath();
          LOGGER.info("Adding input:" + path);
          FileInputFormat.addInputPath(job,path);
          schemaPathMapping.put(path.toString(),sourceName);
        }
      }
      if (!isNested) {
        LOGGER.info("Adding input:" + inputPath);
        FileInputFormat.addInputPath(job,input);
        schemaPathMapping.put(input.toString(),sourceName);
      }
    }
  }
  StringWriter temp=new StringWriter();
  OBJECT_MAPPER.writeValue(temp,schemaPathMapping);
  job.getConfiguration().set("schema.path.mapping",temp.toString());
  temp=new StringWriter();
  OBJECT_MAPPER.writeValue(temp,schemaMap);
  job.getConfiguration().set("schema.json.mapping",temp.toString());
  Path outputPath=new Path(getAndCheck(JOIN_OUTPUT_PATH.toString()));
  if (fs.exists(outputPath)) {
    fs.delete(outputPath,true);
  }
  FileOutputFormat.setOutputPath(job,new Path(getAndCheck(JOIN_OUTPUT_PATH.toString())));
  for (  Object key : props.keySet()) {
    conf.set(key.toString(),props.getProperty(key.toString()));
  }
  job.waitForCompletion(true);
  dumpSummary(job,sourceNames);
  return job;
}
