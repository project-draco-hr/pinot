{
  super();
  this.schema=schema;
  this.segmentName=segmentMetadata.getSegmentName();
  IndexingConfig indexingConfig=tableConfig.getIndexingConfig();
  if (indexingConfig.getSortedColumn().isEmpty()) {
    GLOBAL_LOGGER.info("RealtimeDataResourceZKMetadata contains no information about sorted column for segment {}",segmentName);
    this.sortedColumn=null;
  }
 else {
    String firstSortedColumn=indexingConfig.getSortedColumn().get(0);
    if (this.schema.isExisted(firstSortedColumn)) {
      GLOBAL_LOGGER.info("Setting sorted column name: {} from RealtimeDataResourceZKMetadata for segment {}",firstSortedColumn,segmentName);
      this.sortedColumn=firstSortedColumn;
    }
 else {
      GLOBAL_LOGGER.warn("Sorted column name: {} from RealtimeDataResourceZKMetadata is not existed in schema for segment {}.",firstSortedColumn,segmentName);
      this.sortedColumn=null;
    }
  }
  invertedIndexColumns=indexingConfig.getInvertedIndexColumns();
  this.segmentMetatdaZk=segmentMetadata;
  this.kafkaStreamProviderConfig=new KafkaHighLevelStreamProviderConfig();
  this.kafkaStreamProviderConfig.init(tableConfig,instanceMetadata,schema);
  LOGGER=LoggerFactory.getLogger(RealtimeSegmentDataManager.class.getName() + "_" + segmentName+ "_"+ kafkaStreamProviderConfig.getStreamName());
  LOGGER.info("Created segment data manager with Sorted column:{}, invertedIndexColumns:{}",sortedColumn,invertedIndexColumns);
  segmentEndTimeThreshold=start + kafkaStreamProviderConfig.getTimeThresholdToFlushSegment();
  this.resourceDir=new File(resourceDataDir);
  this.resourceTmpDir=new File(resourceDataDir,"_tmp");
  if (!resourceTmpDir.exists()) {
    resourceTmpDir.mkdirs();
  }
  final String tableName=tableConfig.getTableName();
  this.kafkaStreamProvider=StreamProviderFactory.buildStreamProvider();
  this.kafkaStreamProvider.init(kafkaStreamProviderConfig,tableName,serverMetrics);
  this.kafkaStreamProvider.start();
  LOGGER.info("Started kafka stream provider");
  realtimeSegment=new RealtimeSegmentImpl(schema,kafkaStreamProviderConfig.getSizeThresholdToFlushSegment(),tableName,segmentMetadata.getSegmentName(),kafkaStreamProviderConfig.getStreamName(),serverMetrics);
  realtimeSegment.setSegmentMetadata(segmentMetadata,this.schema);
  notifier=realtimeResourceManager;
  segmentStatusTask=new TimerTask(){
    @Override public void run(){
      computeKeepIndexing();
    }
  }
;
  indexingThread=new Thread(new Runnable(){
    @Override public void run(){
      boolean notFull=true;
      long exceptionSleepMillis=50L;
      LOGGER.info("Starting to collect rows");
      do {
        GenericRow row=null;
        try {
          row=kafkaStreamProvider.next();
          if (row != null) {
            notFull=realtimeSegment.index(row);
            exceptionSleepMillis=50L;
          }
        }
 catch (        Exception e) {
          LOGGER.warn("Caught exception while indexing row, sleeping for {} ms, row contents {}",exceptionSleepMillis,row,e);
          Uninterruptibles.sleepUninterruptibly(exceptionSleepMillis,TimeUnit.MILLISECONDS);
          exceptionSleepMillis=Math.min(60000L,exceptionSleepMillis * 2);
        }
catch (        Error e) {
          LOGGER.error("Caught error in indexing thread",e);
          throw e;
        }
      }
 while (notFull && keepIndexing);
      try {
        LOGGER.info("Indexing threshold reached, proceeding with index conversion");
        segmentStatusTask.cancel();
        LOGGER.info("Indexed {} raw events, current number of docs = {}",realtimeSegment.getRawDocumentCount(),realtimeSegment.getSegmentMetadata().getTotalDocs());
        File tempSegmentFolder=new File(resourceTmpDir,"tmp-" + String.valueOf(System.currentTimeMillis()));
        RealtimeSegmentConverter converter=new RealtimeSegmentConverter(realtimeSegment,tempSegmentFolder.getAbsolutePath(),schema,segmentMetadata.getTableName(),segmentMetadata.getSegmentName(),sortedColumn,invertedIndexColumns);
        LOGGER.info("Trying to build segment {}",segmentName);
        final long buildStartTime=System.nanoTime();
        converter.build();
        final long buildEndTime=System.nanoTime();
        LOGGER.info("Built segment in {}ms",TimeUnit.MILLISECONDS.convert((buildEndTime - buildStartTime),TimeUnit.NANOSECONDS));
        File destDir=new File(resourceDataDir,segmentMetadata.getSegmentName());
        FileUtils.deleteQuietly(destDir);
        FileUtils.moveDirectory(tempSegmentFolder.listFiles()[0],destDir);
        FileUtils.deleteQuietly(tempSegmentFolder);
        long segStartTime=realtimeSegment.getMinTime();
        long segEndTime=realtimeSegment.getMaxTime();
        TimeUnit timeUnit=schema.getTimeFieldSpec().getOutgoingGranularitySpec().getTimeType();
        RealtimeSegmentZKMetadata metadaToOverrite=new RealtimeSegmentZKMetadata();
        metadaToOverrite.setTableName(segmentMetadata.getTableName());
        metadaToOverrite.setSegmentName(segmentMetadata.getSegmentName());
        metadaToOverrite.setSegmentType(SegmentType.OFFLINE);
        metadaToOverrite.setStatus(Status.DONE);
        metadaToOverrite.setStartTime(segStartTime);
        metadaToOverrite.setEndTime(segEndTime);
        metadaToOverrite.setTotalRawDocs(realtimeSegment.getSegmentMetadata().getTotalDocs());
        metadaToOverrite.setTimeUnit(timeUnit);
        Configuration configuration=new PropertyListConfiguration();
        configuration.setProperty(IndexLoadingConfigMetadata.KEY_OF_LOADING_INVERTED_INDEX,invertedIndexColumns);
        IndexLoadingConfigMetadata configMetadata=new IndexLoadingConfigMetadata(configuration);
        IndexSegment segment=Loaders.IndexSegment.load(new File(resourceDir,segmentMetatdaZk.getSegmentName()),mode,configMetadata);
        notifier.notifySegmentCommitted(metadaToOverrite,segment);
        LOGGER.info("Committing Kafka offset");
        boolean commitSuccessful=false;
        try {
          kafkaStreamProvider.commit();
          commitSuccessful=true;
          kafkaStreamProvider.shutdown();
        }
 catch (        Throwable e) {
          LOGGER.error("FATAL: Exception committing or shutting down consumer commitSuccessful={}",commitSuccessful,e);
          if (!commitSuccessful) {
            kafkaStreamProvider.shutdown();
          }
          throw e;
        }
        LOGGER.info("Successfully closed and committed kafka",segmentName);
      }
 catch (      Exception e) {
        LOGGER.error("Caught exception in the realtime indexing thread",e);
      }
    }
  }
);
  indexingThread.start();
  LOGGER.debug("scheduling keepIndexing timer check");
  TimerService.timer.schedule(segmentStatusTask,ONE_MINUTE_IN_MILLSEC,ONE_MINUTE_IN_MILLSEC);
  LOGGER.info("finished scheduling keepIndexing timer check");
}
