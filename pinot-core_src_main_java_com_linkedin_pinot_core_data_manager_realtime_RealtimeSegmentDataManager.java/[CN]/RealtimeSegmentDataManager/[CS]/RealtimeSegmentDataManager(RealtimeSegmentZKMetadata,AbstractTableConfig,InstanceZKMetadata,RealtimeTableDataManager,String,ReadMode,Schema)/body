{
  super();
  this.schema=schema;
  IndexingConfig indexingConfig=tableConfig.getIndexingConfig();
  if (indexingConfig.getSortedColumn().isEmpty()) {
    LOGGER.info("RealtimeDataResourceZKMetadata contains no information about sorted column");
    this.sortedColumn=null;
  }
 else {
    String firstSortedColumn=indexingConfig.getSortedColumn().get(0);
    if (this.schema.isExisted(firstSortedColumn)) {
      LOGGER.info("Setting sorted column name: {} from RealtimeDataResourceZKMetadata.",firstSortedColumn);
      this.sortedColumn=firstSortedColumn;
    }
 else {
      LOGGER.warn("Sorted column name: {} from RealtimeDataResourceZKMetadata is not existed in schema.",firstSortedColumn);
      this.sortedColumn=null;
    }
  }
  invertedIndexColumns=indexingConfig.getInvertedIndexColumns();
  LOGGER.info("Enabling inverted  index for columns name: {}",invertedIndexColumns);
  this.segmentMetatdaZk=segmentMetadata;
  this.segmentName=segmentMetadata.getSegmentName();
  this.kafkaStreamProviderConfig=new KafkaHighLevelStreamProviderConfig();
  this.kafkaStreamProviderConfig.init(tableConfig,instanceMetadata,schema);
  segmentEndTimeThreshold=start + kafkaStreamProviderConfig.getTimeThresholdToFlushSegment();
  this.resourceDir=new File(resourceDataDir);
  this.resourceTmpDir=new File(resourceDataDir,"_tmp");
  if (!resourceTmpDir.exists()) {
    resourceTmpDir.mkdirs();
  }
  this.kafkaStreamProvider=StreamProviderFactory.buildStreamProvider();
  this.kafkaStreamProvider.init(kafkaStreamProviderConfig);
  this.kafkaStreamProvider.start();
  realtimeSegment=new RealtimeSegmentImpl(schema,kafkaStreamProviderConfig.getSizeThresholdToFlushSegment());
  realtimeSegment.setSegmentName(segmentMetadata.getSegmentName());
  realtimeSegment.setSegmentMetadata(segmentMetadata,this.schema);
  notifier=realtimeResourceManager;
  segmentStatusTask=new TimerTask(){
    @Override public void run(){
      computeKeepIndexing();
    }
  }
;
  indexingThread=new Thread(new Runnable(){
    @Override public void run(){
      boolean notFull=true;
      long exceptionSleepMillis=50L;
      LOGGER.info("Starting to collect rows for segment {}",segmentName);
      do {
        GenericRow row=null;
        try {
          row=kafkaStreamProvider.next();
          if (row != null) {
            notFull=realtimeSegment.index(row);
            exceptionSleepMillis=50L;
          }
        }
 catch (        Exception e) {
          LOGGER.warn("Caught exception while indexing row, sleeping for {} ms, row contents {}",exceptionSleepMillis,row,e);
          Uninterruptibles.sleepUninterruptibly(exceptionSleepMillis,TimeUnit.MILLISECONDS);
          exceptionSleepMillis=Math.min(60000L,exceptionSleepMillis * 2);
        }
catch (        Error e) {
          LOGGER.error("Caught error in indexing thread",e);
          throw e;
        }
      }
 while (notFull && keepIndexing);
      try {
        LOGGER.info("Indexing threshold reached, proceeding with index conversion for {}",segmentName);
        segmentStatusTask.cancel();
        LOGGER.info("Trying to persist a realtimeSegment {}" + realtimeSegment.getSegmentName());
        LOGGER.info("Indexed {} raw events, current number of docs = {} for {}",realtimeSegment.getRawDocumentCount(),realtimeSegment.getSegmentMetadata().getTotalDocs(),segmentName);
        File tempSegmentFolder=new File(resourceTmpDir,"tmp-" + String.valueOf(System.currentTimeMillis()));
        RealtimeSegmentConverter converter=new RealtimeSegmentConverter(realtimeSegment,tempSegmentFolder.getAbsolutePath(),schema,segmentMetadata.getTableName(),segmentMetadata.getSegmentName(),sortedColumn,invertedIndexColumns);
        LOGGER.info("Trying to build segment {}",segmentName);
        converter.build();
        File destDir=new File(resourceDataDir,segmentMetadata.getSegmentName());
        FileUtils.deleteQuietly(destDir);
        FileUtils.moveDirectory(tempSegmentFolder.listFiles()[0],destDir);
        FileUtils.deleteQuietly(tempSegmentFolder);
        long startTime=realtimeSegment.getMinTime();
        long endTime=realtimeSegment.getMaxTime();
        TimeUnit timeUnit=schema.getTimeFieldSpec().getOutgoingGranularitySpec().getTimeType();
        RealtimeSegmentZKMetadata metadaToOverrite=new RealtimeSegmentZKMetadata();
        metadaToOverrite.setTableName(segmentMetadata.getTableName());
        metadaToOverrite.setSegmentName(segmentMetadata.getSegmentName());
        metadaToOverrite.setSegmentType(SegmentType.OFFLINE);
        metadaToOverrite.setStatus(Status.DONE);
        metadaToOverrite.setStartTime(startTime);
        metadaToOverrite.setEndTime(endTime);
        metadaToOverrite.setTotalRawDocs(realtimeSegment.getSegmentMetadata().getTotalDocs());
        metadaToOverrite.setTimeUnit(timeUnit);
        Configuration configuration=new PropertyListConfiguration();
        configuration.setProperty(IndexLoadingConfigMetadata.KEY_OF_LOADING_INVERTED_INDEX,invertedIndexColumns);
        IndexLoadingConfigMetadata configMetadata=new IndexLoadingConfigMetadata(configuration);
        IndexSegment segment=Loaders.IndexSegment.load(new File(resourceDir,segmentMetatdaZk.getSegmentName()),mode,configMetadata);
        notifier.notifySegmentCommitted(metadaToOverrite,segment);
        boolean commitSuccessful=false;
        try {
          kafkaStreamProvider.commit();
          commitSuccessful=true;
          kafkaStreamProvider.shutdown();
        }
 catch (        Throwable e) {
          LOGGER.error("FATAL: Exception committing or shutting down consumer for segment {} table {}:commitSuccessful={}",segmentName,segmentMetadata.getTableName(),commitSuccessful,e);
          if (!commitSuccessful) {
            kafkaStreamProvider.shutdown();
          }
          throw e;
        }
        LOGGER.info("Successfully closed and committed kafka for {}",segmentName);
      }
 catch (      Exception e) {
        LOGGER.error("Caught exception in the realtime indexing thread for {}",segmentName,e);
      }
    }
  }
);
  indexingThread.start();
  LOGGER.debug("scheduling keepIndexing timer check");
  TimerService.timer.schedule(segmentStatusTask,ONE_MINUTE_IN_MILLSEC,ONE_MINUTE_IN_MILLSEC);
  LOGGER.debug("finished scheduling keepIndexing timer check");
}
