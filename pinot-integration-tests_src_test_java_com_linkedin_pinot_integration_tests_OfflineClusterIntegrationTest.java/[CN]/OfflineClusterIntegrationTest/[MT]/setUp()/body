{
  startZk();
  startController();
  startBroker();
  startOfflineServer();
  createResource("myresource");
  addTableToResource("myresource","mytable");
  TarGzCompressionUtils.unTar(new File(TestUtils.getFileFromResourceUrl(OfflineClusterIntegrationTest.class.getClassLoader().getResource("On_Time_On_Time_Performance_2014_100k_subset.tar.gz"))),new File("/tmp/OfflineClusterIntegrationTest"));
  _tmpDir.mkdirs();
  ExecutorService executor=Executors.newCachedThreadPool();
  Class.forName("org.h2.Driver");
  _connection=DriverManager.getConnection("jdbc:h2:mem:");
  executor.execute(new Runnable(){
    @Override public void run(){
      try {
        File avroFile=new File(_tmpDir.getPath() + "/On_Time_On_Time_Performance_2014_1.avro");
        DatumReader<GenericRecord> datumReader=new GenericDatumReader<GenericRecord>();
        DataFileReader<GenericRecord> dataFileReader=new DataFileReader<GenericRecord>(avroFile,datumReader);
        Schema schema=dataFileReader.getSchema();
        List<Schema.Field> fields=schema.getFields();
        String[] columnNamesAndTypes=new String[fields.size()];
        int index=0;
        for (        Schema.Field field : fields) {
          List<Schema> types=field.schema().getTypes();
          if (types.size() == 1) {
            columnNamesAndTypes[index]=field.name() + " " + types.get(0).getName()+ " not null";
          }
 else {
            columnNamesAndTypes[index]=field.name() + " " + types.get(0).getName();
          }
          columnNamesAndTypes[index]=columnNamesAndTypes[index].replace("string","varchar(128)");
          ++index;
        }
        _connection.prepareCall("create table mytable(" + StringUtil.join(",",columnNamesAndTypes) + ")").execute();
        long start=System.currentTimeMillis();
        StringBuilder params=new StringBuilder("?");
        for (int i=0; i < columnNamesAndTypes.length - 1; i++) {
          params.append(",?");
        }
        PreparedStatement statement=_connection.prepareStatement("INSERT INTO mytable VALUES (" + params.toString() + ")");
        dataFileReader.close();
        for (int currentSegmentIndex=1; currentSegmentIndex <= SEGMENT_COUNT; ++currentSegmentIndex) {
          avroFile=new File(_tmpDir.getPath() + "/On_Time_On_Time_Performance_2014_" + currentSegmentIndex+ ".avro");
          datumReader=new GenericDatumReader<GenericRecord>();
          dataFileReader=new DataFileReader<GenericRecord>(avroFile,datumReader);
          GenericRecord record=null;
          while (dataFileReader.hasNext()) {
            record=dataFileReader.next(record);
            for (int i=0; i < columnNamesAndTypes.length; i++) {
              Object value=record.get(i);
              if (value instanceof Utf8) {
                value=value.toString();
              }
              statement.setObject(i + 1,value);
            }
            statement.execute();
          }
          dataFileReader.close();
        }
        System.out.println("Insertion took " + (System.currentTimeMillis() - start));
      }
 catch (      SQLException e) {
        e.printStackTrace();
      }
catch (      IOException e) {
        e.printStackTrace();
      }
    }
  }
);
  System.out.println("Building " + SEGMENT_COUNT + " segments in parallel");
  for (int i=1; i <= SEGMENT_COUNT; ++i) {
    final int segmentNumber=i;
    executor.execute(new Runnable(){
      @Override public void run(){
        try {
          System.out.println("Starting to build segment " + segmentNumber);
          File outputDir=new File(_tmpDir,"segment-" + segmentNumber);
          final SegmentGeneratorConfig genConfig=SegmentTestUtils.getSegmentGenSpecWithSchemAndProjectedColumns(new File(_tmpDir.getPath() + "/On_Time_On_Time_Performance_2014_" + segmentNumber+ ".avro"),outputDir,"daysSinceEpoch",TimeUnit.DAYS,"myresource","mytable");
          genConfig.setSegmentNamePostfix(Integer.toString(segmentNumber));
          final SegmentIndexCreationDriver driver=SegmentCreationDriverFactory.get(null);
          driver.init(genConfig);
          driver.build();
          TarGzCompressionUtils.createTarGzOfDirectory(outputDir.getAbsolutePath() + "/myresource_mytable_" + segmentNumber,new File(outputDir.getParent(),"myresource_mytable_" + segmentNumber).getAbsolutePath());
          System.out.println("Completed segment " + segmentNumber);
        }
 catch (        Exception e) {
          throw new RuntimeException(e);
        }
      }
    }
);
  }
  executor.shutdown();
  executor.awaitTermination(10,TimeUnit.MINUTES);
  final CountDownLatch latch=new CountDownLatch(1);
  HelixManager manager=HelixManagerFactory.getZKHelixManager(getHelixClusterName(),"test_instance",InstanceType.SPECTATOR,ZkTestUtils.DEFAULT_ZK_STR);
  manager.connect();
  manager.addExternalViewChangeListener(new ExternalViewChangeListener(){
    @Override public void onExternalViewChange(    List<ExternalView> externalViewList,    NotificationContext changeContext){
      for (      ExternalView externalView : externalViewList) {
        if (externalView.getId().contains("myresource")) {
          Set<String> partitionSet=externalView.getPartitionSet();
          if (partitionSet.size() == SEGMENT_COUNT) {
            int onlinePartitionCount=0;
            for (            String partitionId : partitionSet) {
              Map<String,String> partitionStateMap=externalView.getStateMap(partitionId);
              if (partitionStateMap.containsValue("ONLINE")) {
                onlinePartitionCount++;
              }
            }
            if (onlinePartitionCount == SEGMENT_COUNT) {
              System.out.println("Got " + SEGMENT_COUNT + " online resources, unlatching the main thread");
              latch.countDown();
            }
          }
        }
      }
    }
  }
);
  for (int i=1; i <= SEGMENT_COUNT; ++i) {
    System.out.println("Uploading segment " + i);
    File file=new File(_tmpDir,"myresource_mytable_" + i);
    FileUploadUtils.sendFile("localhost","8998","myresource_mytable_" + i,new FileInputStream(file),file.length());
  }
  latch.await();
}
