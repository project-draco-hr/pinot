{
  super();
  _realtimeTableDataManager=realtimeTableDataManager;
  final String segmentVersionStr=tableConfig.getIndexingConfig().getSegmentFormatVersion();
  _segmentVersion=SegmentVersion.fromStringOrDefault(segmentVersionStr);
  this.schema=schema;
  this.extractor=(PlainFieldExtractor)FieldExtractorFactory.getPlainFieldExtractor(schema);
  this.serverMetrics=serverMetrics;
  this.segmentName=segmentMetadata.getSegmentName();
  this.tableName=tableConfig.getTableName();
  IndexingConfig indexingConfig=tableConfig.getIndexingConfig();
  if (indexingConfig.getSortedColumn().isEmpty()) {
    LOGGER.info("RealtimeDataResourceZKMetadata contains no information about sorted column for segment {}",segmentName);
    this.sortedColumn=null;
  }
 else {
    String firstSortedColumn=indexingConfig.getSortedColumn().get(0);
    if (this.schema.hasColumn(firstSortedColumn)) {
      LOGGER.info("Setting sorted column name: {} from RealtimeDataResourceZKMetadata for segment {}",firstSortedColumn,segmentName);
      this.sortedColumn=firstSortedColumn;
    }
 else {
      LOGGER.warn("Sorted column name: {} from RealtimeDataResourceZKMetadata is not existed in schema for segment {}.",firstSortedColumn,segmentName);
      this.sortedColumn=null;
    }
  }
  invertedIndexColumns=indexingConfig.getInvertedIndexColumns();
  if (sortedColumn != null && !invertedIndexColumns.contains(sortedColumn)) {
    invertedIndexColumns.add(sortedColumn);
  }
  this.segmentMetatdaZk=segmentMetadata;
  this.kafkaStreamProviderConfig=new KafkaHighLevelStreamProviderConfig();
  this.kafkaStreamProviderConfig.init(tableConfig,instanceMetadata,schema);
  segmentLogger=LoggerFactory.getLogger(HLRealtimeSegmentDataManager.class.getName() + "_" + segmentName+ "_"+ kafkaStreamProviderConfig.getStreamName());
  segmentLogger.info("Created segment data manager with Sorted column:{}, invertedIndexColumns:{}",sortedColumn,invertedIndexColumns);
  segmentEndTimeThreshold=start + kafkaStreamProviderConfig.getTimeThresholdToFlushSegment();
  this.resourceDir=new File(resourceDataDir);
  this.resourceTmpDir=new File(resourceDataDir,"_tmp");
  if (!resourceTmpDir.exists()) {
    resourceTmpDir.mkdirs();
  }
  final String tableName=tableConfig.getTableName();
  this.kafkaStreamProvider=StreamProviderFactory.buildStreamProvider();
  this.kafkaStreamProvider.init(kafkaStreamProviderConfig,tableName,serverMetrics);
  this.kafkaStreamProvider.start();
  this.tableStreamName=tableName + "_" + kafkaStreamProviderConfig.getStreamName();
  segmentLogger.info("Started kafka stream provider");
  realtimeSegment=new RealtimeSegmentImpl(schema,kafkaStreamProviderConfig.getSizeThresholdToFlushSegment(),tableName,segmentMetadata.getSegmentName(),kafkaStreamProviderConfig.getStreamName(),serverMetrics,invertedIndexColumns);
  realtimeSegment.setSegmentMetadata(segmentMetadata,this.schema);
  notifier=realtimeTableDataManager;
  segmentStatusTask=new TimerTask(){
    @Override public void run(){
      computeKeepIndexing();
    }
  }
;
  indexingThread=new Thread(new Runnable(){
    @Override public void run(){
      boolean notFull=true;
      long exceptionSleepMillis=50L;
      segmentLogger.info("Starting to collect rows");
      do {
        GenericRow row=null;
        try {
          row=kafkaStreamProvider.next();
          if (row != null) {
            row=extractor.transform(row);
            notFull=realtimeSegment.index(row);
            exceptionSleepMillis=50L;
          }
        }
 catch (        Exception e) {
          segmentLogger.warn("Caught exception while indexing row, sleeping for {} ms, row contents {}",exceptionSleepMillis,row,e);
          Uninterruptibles.sleepUninterruptibly(exceptionSleepMillis,TimeUnit.MILLISECONDS);
          exceptionSleepMillis=Math.min(60000L,exceptionSleepMillis * 2);
        }
catch (        Error e) {
          segmentLogger.error("Caught error in indexing thread",e);
          throw e;
        }
      }
 while (notFull && keepIndexing && (!isShuttingDown));
      if (isShuttingDown) {
        segmentLogger.info("Shutting down indexing thread!");
        return;
      }
      try {
        int numErrors, numConversions, numNulls, numNullCols;
        if ((numErrors=extractor.getTotalErrors()) > 0) {
          serverMetrics.addMeteredTableValue(tableStreamName,ServerMeter.ROWS_WITH_ERRORS,(long)numErrors);
        }
        Map<String,Integer> errorCount=extractor.getErrorCount();
        for (        String column : errorCount.keySet()) {
          if ((numErrors=errorCount.get(column)) > 0) {
            segmentLogger.warn("Column {} had {} rows with errors",column,numErrors);
          }
        }
        if ((numConversions=extractor.getTotalConversions()) > 0) {
          serverMetrics.addMeteredTableValue(tableStreamName,ServerMeter.ROWS_NEEDING_CONVERSIONS,(long)numConversions);
          segmentLogger.info("{} rows needed conversions ",numConversions);
        }
        if ((numNulls=extractor.getTotalNulls()) > 0) {
          serverMetrics.addMeteredTableValue(tableStreamName,ServerMeter.ROWS_WITH_NULL_VALUES,(long)numNulls);
          segmentLogger.info("{} rows had null columns",numNulls);
        }
        if ((numNullCols=extractor.getTotalNullCols()) > 0) {
          serverMetrics.addMeteredTableValue(tableStreamName,ServerMeter.COLUMNS_WITH_NULL_VALUES,(long)numNullCols);
          segmentLogger.info("{} columns had null values",numNullCols);
        }
        segmentLogger.info("Indexing threshold reached, proceeding with index conversion");
        segmentStatusTask.cancel();
        updateCurrentDocumentCountMetrics();
        segmentLogger.info("Indexed {} raw events, current number of docs = {}",realtimeSegment.getRawDocumentCount(),realtimeSegment.getSegmentMetadata().getTotalDocs());
        File tempSegmentFolder=new File(resourceTmpDir,"tmp-" + String.valueOf(System.currentTimeMillis()));
        RealtimeSegmentConverter converter=new RealtimeSegmentConverter(realtimeSegment,tempSegmentFolder.getAbsolutePath(),schema,segmentMetadata.getTableName(),segmentMetadata.getSegmentName(),sortedColumn,invertedIndexColumns);
        segmentLogger.info("Trying to build segment");
        final long buildStartTime=System.nanoTime();
        converter.build(_segmentVersion);
        final long buildEndTime=System.nanoTime();
        segmentLogger.info("Built segment in {} ms",TimeUnit.MILLISECONDS.convert((buildEndTime - buildStartTime),TimeUnit.NANOSECONDS));
        File destDir=new File(resourceDataDir,segmentMetadata.getSegmentName());
        FileUtils.deleteQuietly(destDir);
        FileUtils.moveDirectory(tempSegmentFolder.listFiles()[0],destDir);
        FileUtils.deleteQuietly(tempSegmentFolder);
        long segStartTime=realtimeSegment.getMinTime();
        long segEndTime=realtimeSegment.getMaxTime();
        TimeUnit timeUnit=schema.getTimeFieldSpec().getOutgoingGranularitySpec().getTimeType();
        Configuration configuration=new PropertyListConfiguration();
        configuration.setProperty(IndexLoadingConfigMetadata.KEY_OF_LOADING_INVERTED_INDEX,invertedIndexColumns);
        IndexLoadingConfigMetadata configMetadata=new IndexLoadingConfigMetadata(configuration);
        IndexSegment segment=Loaders.IndexSegment.load(new File(resourceDir,segmentMetatdaZk.getSegmentName()),mode,configMetadata);
        segmentLogger.info("Committing Kafka offsets");
        boolean commitSuccessful=false;
        try {
          kafkaStreamProvider.commit();
          commitSuccessful=true;
          kafkaStreamProvider.shutdown();
          segmentLogger.info("Successfully committed Kafka offsets, consumer release requested.");
        }
 catch (        Throwable e) {
          segmentLogger.error("FATAL: Exception committing or shutting down consumer commitSuccessful={}",commitSuccessful,e);
          serverMetrics.addMeteredTableValue(tableName,ServerMeter.REALTIME_OFFSET_COMMIT_EXCEPTIONS,1L);
          if (!commitSuccessful) {
            kafkaStreamProvider.shutdown();
          }
        }
        try {
          segmentLogger.info("Marking current segment as completed in Helix");
          RealtimeSegmentZKMetadata metadataToOverwrite=new RealtimeSegmentZKMetadata();
          metadataToOverwrite.setTableName(segmentMetadata.getTableName());
          metadataToOverwrite.setSegmentName(segmentMetadata.getSegmentName());
          metadataToOverwrite.setSegmentType(SegmentType.OFFLINE);
          metadataToOverwrite.setStatus(Status.DONE);
          metadataToOverwrite.setStartTime(segStartTime);
          metadataToOverwrite.setEndTime(segEndTime);
          metadataToOverwrite.setTotalRawDocs(realtimeSegment.getSegmentMetadata().getTotalDocs());
          metadataToOverwrite.setTimeUnit(timeUnit);
          notifier.notifySegmentCommitted(metadataToOverwrite,segment);
          segmentLogger.info("Completed write of segment completion to Helix, waiting for controller to assign a new segment");
        }
 catch (        Exception e) {
          if (commitSuccessful) {
            segmentLogger.error("Offsets were committed to Kafka but we were unable to mark this segment as completed in Helix. Manually mark the segment as completed in Helix; restarting this instance will result in data loss.",e);
          }
 else {
            segmentLogger.warn("Caught exception while marking segment as completed in Helix. Offsets were not written, restarting the instance should be safe.",e);
          }
        }
      }
 catch (      Exception e) {
        segmentLogger.error("Caught exception in the realtime indexing thread",e);
      }
    }
  }
);
  indexingThread.start();
  serverMetrics.addValueToTableGauge(tableName,ServerGauge.SEGMENT_COUNT,1L);
  segmentLogger.debug("scheduling keepIndexing timer check");
  TimerService.timer.schedule(segmentStatusTask,ONE_MINUTE_IN_MILLSEC,ONE_MINUTE_IN_MILLSEC);
  segmentLogger.info("finished scheduling keepIndexing timer check");
}
