{
  Job job=Job.getInstance(getConf());
  job.setJarByClass(SegmentCreationPhaseJob.class);
  job.setJobName(name);
  FileSystem fs=FileSystem.get(getConf());
  Configuration configuration=job.getConfiguration();
  String inputSegmentDir=getAndSetConfiguration(configuration,SEGMENT_CREATION_INPUT_PATH);
  LOGGER.info("Input path : {}",inputSegmentDir);
  Schema avroSchema=ThirdeyeAvroUtils.getSchema(inputSegmentDir);
  LOGGER.info("Schema : {}",avroSchema);
  String metricTypesProperty=ThirdeyeAvroUtils.getMetricTypesProperty(props.getProperty(ThirdEyeConfigProperties.THIRDEYE_METRIC_NAMES.toString()),avroSchema);
  props.setProperty(ThirdEyeConfigProperties.THIRDEYE_METRIC_TYPES.toString(),metricTypesProperty);
  ThirdEyeConfig thirdeyeConfig=ThirdEyeConfig.fromProperties(props);
  LOGGER.info("ThirdEyeConfig {}",thirdeyeConfig.encode());
  String outputDir=getAndSetConfiguration(configuration,SEGMENT_CREATION_OUTPUT_PATH);
  LOGGER.info("Output path : {}",outputDir);
  Path stagingDir=new Path(outputDir,TEMP);
  LOGGER.info("Staging dir : {}",stagingDir);
  String segmentWallClockStart=getAndSetConfiguration(configuration,SEGMENT_CREATION_WALLCLOCK_START_TIME);
  LOGGER.info("Segment wallclock start time : {}",segmentWallClockStart);
  String segmentWallClockEnd=getAndSetConfiguration(configuration,SEGMENT_CREATION_WALLCLOCK_END_TIME);
  LOGGER.info("Segment wallclock end time : {}",segmentWallClockEnd);
  String schedule=getAndSetConfiguration(configuration,SEGMENT_CREATION_SCHEDULE);
  LOGGER.info("Segment schedule : {}",schedule);
  if (fs.exists(stagingDir)) {
    LOGGER.warn("Found the temp folder, deleting it");
    fs.delete(stagingDir,true);
  }
  fs.mkdirs(stagingDir);
  fs.mkdirs(new Path(stagingDir + "/input/"));
  if (fs.exists(new Path(outputDir))) {
    LOGGER.warn("Found the output folder deleting it");
    fs.delete(new Path(outputDir),true);
  }
  fs.mkdirs(new Path(outputDir));
  List<FileStatus> inputDataFiles=new ArrayList<>();
  for (  String input : inputSegmentDir.split(",")) {
    Path inputPathPattern=new Path(input);
    inputDataFiles.addAll(Arrays.asList(fs.listStatus(inputPathPattern)));
  }
  LOGGER.info("size {}",inputDataFiles.size());
  try {
    for (int seqId=0; seqId < inputDataFiles.size(); ++seqId) {
      FileStatus file=inputDataFiles.get(seqId);
      String completeFilePath=" " + file.getPath().toString() + " "+ seqId;
      Path newOutPutFile=new Path((stagingDir + "/input/" + file.getPath().toString().replace('.','_').replace('/','_').replace(':','_')+ ".txt"));
      FSDataOutputStream stream=fs.create(newOutPutFile);
      LOGGER.info("wrote {}",completeFilePath);
      stream.writeUTF(completeFilePath);
      stream.flush();
      stream.close();
    }
  }
 catch (  Exception e) {
    LOGGER.error("Exception while reading input files ",e);
  }
  job.setMapperClass(SegmentCreationPhaseMapReduceJob.SegmentCreationMapper.class);
  if (System.getenv("HADOOP_TOKEN_FILE_LOCATION") != null) {
    job.getConfiguration().set("mapreduce.job.credentials.binary",System.getenv("HADOOP_TOKEN_FILE_LOCATION"));
  }
  job.setInputFormatClass(TextInputFormat.class);
  job.setOutputFormatClass(TextOutputFormat.class);
  job.setMapOutputKeyClass(LongWritable.class);
  job.setMapOutputValueClass(Text.class);
  FileInputFormat.addInputPath(job,new Path(stagingDir + "/input/"));
  FileOutputFormat.setOutputPath(job,new Path(stagingDir + "/output/"));
  job.getConfiguration().setInt(JobContext.NUM_MAPS,inputDataFiles.size());
  job.getConfiguration().set(SEGMENT_CREATION_THIRDEYE_CONFIG.toString(),OBJECT_MAPPER.writeValueAsString(thirdeyeConfig));
  job.setMaxReduceAttempts(1);
  job.setMaxMapAttempts(0);
  job.setNumReduceTasks(0);
  for (  Object key : props.keySet()) {
    job.getConfiguration().set(key.toString(),props.getProperty(key.toString()));
  }
  job.waitForCompletion(true);
  if (!job.isSuccessful()) {
    throw new RuntimeException("Job failed : " + job);
  }
  LOGGER.info("Moving Segment Tar files from {} to: {}",stagingDir + "/output/segmentTar",outputDir);
  FileStatus[] segmentArr=fs.listStatus(new Path(stagingDir + "/output/segmentTar"));
  for (  FileStatus segment : segmentArr) {
    fs.rename(segment.getPath(),new Path(outputDir,segment.getPath().getName()));
  }
  LOGGER.info("Cleanup the working directory.");
  LOGGER.info("Deleting the dir: {}",stagingDir);
  fs.delete(stagingDir,true);
  return job;
}
