{
  ensureDirectoryExistsAndIsEmpty(_tmpDir);
  ensureDirectoryExistsAndIsEmpty(_segmentDir);
  ensureDirectoryExistsAndIsEmpty(_tarDir);
  startHybridCluster();
  TarGzCompressionUtils.unTar(new File(TestUtils.getFileFromResourceUrl(OfflineClusterIntegrationTest.class.getClassLoader().getResource("On_Time_On_Time_Performance_2014_100k_subset_nonulls.tar.gz"))),_tmpDir);
  _tmpDir.mkdirs();
  final List<File> avroFiles=getAllAvroFiles();
  File schemaFile=getSchemaFile();
  schema=Schema.fromFile(schemaFile);
  addSchema(schemaFile,schema.getSchemaName());
  final List<String> invertedIndexColumns=makeInvertedIndexColumns();
  final String sortedColumn=makeSortedColumn();
  addHybridTable("mytable","DaysSinceEpoch","daysSinceEpoch",KafkaStarterUtils.DEFAULT_ZK_STR,KAFKA_TOPIC,schema.getSchemaName(),TENANT_NAME,TENANT_NAME,avroFiles.get(0),sortedColumn,invertedIndexColumns);
  LOGGER.info("Running with Sorted column=" + sortedColumn + " and inverted index columns = "+ invertedIndexColumns);
  final List<File> offlineAvroFiles=getOfflineAvroFiles(avroFiles);
  final List<File> realtimeAvroFiles=getRealtimeAvroFiles(avroFiles);
  ExecutorService executor=Executors.newCachedThreadPool();
  setupH2AndInsertAvro(avroFiles,executor);
  LOGGER.info("Creating offline segments from avro files " + offlineAvroFiles);
  buildSegmentsFromAvro(offlineAvroFiles,executor,0,_segmentDir,_tarDir,"mytable",false);
  setupQueryGenerator(avroFiles,executor);
  executor.shutdown();
  executor.awaitTermination(10,TimeUnit.MINUTES);
  final CountDownLatch latch=new CountDownLatch(1);
  HelixManager manager=HelixManagerFactory.getZKHelixManager(getHelixClusterName(),"test_instance",InstanceType.SPECTATOR,ZkStarter.DEFAULT_ZK_STR);
  manager.connect();
  manager.addExternalViewChangeListener(new ExternalViewChangeListener(){
    @Override public void onExternalViewChange(    List<ExternalView> externalViewList,    NotificationContext changeContext){
      for (      ExternalView externalView : externalViewList) {
        if (externalView.getId().contains("mytable")) {
          Set<String> partitionSet=externalView.getPartitionSet();
          if (partitionSet.size() == offlineSegmentCount) {
            int onlinePartitionCount=0;
            for (            String partitionId : partitionSet) {
              Map<String,String> partitionStateMap=externalView.getStateMap(partitionId);
              if (partitionStateMap.containsValue("ONLINE")) {
                onlinePartitionCount++;
              }
            }
            if (onlinePartitionCount == offlineSegmentCount) {
              System.out.println("Got " + offlineSegmentCount + " online tables, unlatching the main thread");
              latch.countDown();
            }
          }
        }
      }
    }
  }
);
  int i=0;
  for (  String segmentName : _tarDir.list()) {
    System.out.println("Uploading segment " + (i++) + " : "+ segmentName);
    File file=new File(_tarDir,segmentName);
    FileUploadUtils.sendSegmentFile("localhost","8998",segmentName,new FileInputStream(file),file.length());
  }
  latch.await();
  LOGGER.info("Pushing data from realtime avro files " + realtimeAvroFiles);
  pushAvroIntoKafka(realtimeAvroFiles,KafkaStarterUtils.DEFAULT_KAFKA_BROKER,KAFKA_TOPIC);
  int pinotRecordCount, h2RecordCount;
  long timeInTwoMinutes=System.currentTimeMillis() + 2 * 60 * 1000L;
  Statement statement=_connection.createStatement(ResultSet.TYPE_FORWARD_ONLY,ResultSet.CONCUR_READ_ONLY);
  statement.execute("select count(*) from mytable");
  ResultSet rs=statement.getResultSet();
  rs.first();
  h2RecordCount=rs.getInt(1);
  rs.close();
  waitForRecordCountToStabilizeToExpectedCount(h2RecordCount,timeInTwoMinutes);
}
