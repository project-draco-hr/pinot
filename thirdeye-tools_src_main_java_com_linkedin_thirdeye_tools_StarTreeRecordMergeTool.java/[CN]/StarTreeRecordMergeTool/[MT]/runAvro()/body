{
  Set<Long> timeBuckets=new HashSet<Long>();
  Map<Map<String,String>,Map<String,Integer>> aggregates=new HashMap<Map<String,String>,Map<String,Integer>>();
  Schema schema=null;
  int numRead=0;
  for (  File inputFile : inputFiles) {
    LOG.info("Processing {}",inputFile);
    FileReader<GenericRecord> fileReader=DataFileReader.openReader(inputFile,new GenericDatumReader<GenericRecord>());
    if (schema == null) {
      schema=fileReader.getSchema();
      for (      DimensionSpec dimensionSpec : config.getDimensions()) {
        Schema.Field field=schema.getField(dimensionSpec.getName());
        if (field == null) {
          throw new IllegalStateException("Schema missing field " + dimensionSpec.getName() + ": "+ schema);
        }
      }
      for (      MetricSpec spec : config.getMetrics()) {
        Schema.Field field=schema.getField(spec.getName());
        if (field == null) {
          throw new IllegalStateException("Schema missing field " + spec.getName() + ": "+ schema);
        }
      }
      Schema.Field field=schema.getField(config.getTime().getColumnName());
      if (field == null) {
        throw new IllegalStateException("Schema missing field " + config.getTime().getColumnName() + ": "+ schema);
      }
      LOG.info("Using schema {}",schema);
    }
 else     if (!schema.equals(fileReader.getSchema())) {
      throw new IllegalStateException("Cannot merge records with different schemas: " + schema + ";"+ fileReader.getSchema());
    }
    GenericRecord record=null;
    while (fileReader.hasNext()) {
      record=fileReader.next(record);
      Map<String,String> dimensionValues=new HashMap<String,String>(config.getDimensions().size());
      for (      DimensionSpec dimensionSpec : config.getDimensions()) {
        Object dimensionValue=record.get(dimensionSpec.getName());
        if (dimensionValue == null) {
          throw new IllegalStateException("Got null value for " + dimensionSpec.getName() + ": "+ record);
        }
        dimensionValues.put(dimensionSpec.getName(),dimensionValue.toString());
      }
      Map<String,Integer> metricValues=aggregates.get(dimensionValues);
      if (metricValues == null) {
        metricValues=new HashMap<String,Integer>();
        for (        MetricSpec spec : config.getMetrics()) {
          metricValues.put(spec.getName(),0);
        }
        aggregates.put(dimensionValues,metricValues);
      }
      for (      MetricSpec spec : config.getMetrics()) {
        Object metricValue=record.get(spec.getName());
        if (metricValue == null) {
          metricValue=0;
        }
        Integer newValue=metricValues.get(spec.getName()) + ((Number)metricValue).intValue();
        metricValues.put(spec.getName(),newValue);
      }
      Object timeValue=record.get(config.getTime().getColumnName());
      if (timeValue != null) {
        timeBuckets.add(((Number)timeValue).longValue());
      }
      if (++numRead % 5000 == 0) {
        LOG.info("Read {} records ({} dimension combinations) ({} time buckets)",numRead,aggregates.size(),timeBuckets.size());
      }
    }
  }
  if (schema == null) {
    throw new IllegalStateException("Did not obtain schema from input file");
  }
  LOG.info("Writing aggregates to {}",outputFile);
  DataFileWriter<GenericRecord> fileWriter=new DataFileWriter<GenericRecord>(new GenericDatumWriter<GenericRecord>(schema));
  fileWriter.create(schema,outputFile);
  int numWritten=0;
  GenericRecord record=new GenericData.Record(schema);
  for (  Map.Entry<Map<String,String>,Map<String,Integer>> entry : aggregates.entrySet()) {
    for (    Map.Entry<String,String> dimension : entry.getKey().entrySet()) {
      Schema.Field field=schema.getField(dimension.getKey());
switch (getType(field.schema())) {
case INT:
        record.put(dimension.getKey(),Integer.valueOf(dimension.getValue()));
      break;
case LONG:
    record.put(dimension.getKey(),Long.valueOf(dimension.getValue()));
  break;
case FLOAT:
record.put(dimension.getKey(),Float.valueOf(dimension.getValue()));
break;
case DOUBLE:
record.put(dimension.getKey(),Double.valueOf(dimension.getValue()));
break;
case BOOLEAN:
record.put(dimension.getKey(),Boolean.valueOf(dimension.getValue()));
break;
case STRING:
record.put(dimension.getKey(),dimension.getValue());
break;
default :
throw new IllegalStateException("Unsupported dimension type " + field.schema());
}
}
for (Map.Entry<String,Integer> metric : entry.getValue().entrySet()) {
Schema.Field field=schema.getField(metric.getKey());
switch (getType(field.schema())) {
case INT:
record.put(metric.getKey(),metric.getValue());
break;
case LONG:
record.put(metric.getKey(),metric.getValue().longValue());
break;
case FLOAT:
record.put(metric.getKey(),metric.getValue().floatValue());
break;
case DOUBLE:
record.put(metric.getKey(),metric.getValue().doubleValue());
break;
default :
throw new IllegalStateException("Invalid metric schema type: " + field.schema().getType());
}
}
for (Schema.Field field : schema.getFields()) {
if (!entry.getKey().containsKey(field.name()) && !entry.getValue().containsKey(field.name())) {
switch (getType(field.schema())) {
case INT:
record.put(field.name(),0);
break;
case LONG:
record.put(field.name(),0L);
break;
case FLOAT:
case DOUBLE:
record.put(field.name(),0.0);
break;
default :
throw new IllegalStateException("Invalid time schema type: " + field.schema().getType());
}
}
}
fileWriter.append(record);
if (++numWritten % 5000 == 0) {
LOG.info("Wrote {} records to output file",numWritten);
}
}
fileWriter.close();
Long minTimeBucket=null;
Long maxTimeBucket=null;
for (Long timeBucket : timeBuckets) {
if (minTimeBucket == null || minTimeBucket > timeBucket) {
minTimeBucket=timeBucket;
}
if (maxTimeBucket == null || maxTimeBucket < timeBucket) {
maxTimeBucket=timeBucket;
}
}
LOG.info("Read {} records; wrote {} records",numRead,numWritten);
LOG.info("Processed {} time buckets (min={}, max={})",timeBuckets.size(),minTimeBucket,maxTimeBucket);
}
