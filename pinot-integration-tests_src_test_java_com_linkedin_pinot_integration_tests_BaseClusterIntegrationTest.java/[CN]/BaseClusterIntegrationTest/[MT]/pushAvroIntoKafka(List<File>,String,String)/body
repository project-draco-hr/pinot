{
  Properties properties=new Properties();
  properties.put("metadata.broker.list",kafkaBroker);
  properties.put("serializer.class","kafka.serializer.DefaultEncoder");
  properties.put("request.required.acks","1");
  ProducerConfig producerConfig=new ProducerConfig(properties);
  Producer<String,byte[]> producer=new Producer<String,byte[]>(producerConfig);
  for (  File avroFile : avroFiles) {
    try {
      ByteArrayOutputStream outputStream=new ByteArrayOutputStream(65536);
      DataFileStream<GenericRecord> reader=AvroUtils.getAvroReader(avroFile);
      BinaryEncoder binaryEncoder=new EncoderFactory().directBinaryEncoder(outputStream,null);
      GenericDatumWriter<GenericRecord> datumWriter=new GenericDatumWriter<GenericRecord>(reader.getSchema());
      int recordCount=0;
      List<KeyedMessage<String,byte[]>> messagesToWrite=new ArrayList<KeyedMessage<String,byte[]>>(10000);
      for (      GenericRecord genericRecord : reader) {
        outputStream.reset();
        datumWriter.write(genericRecord,binaryEncoder);
        binaryEncoder.flush();
        byte[] bytes=outputStream.toByteArray();
        KeyedMessage<String,byte[]> data=new KeyedMessage<String,byte[]>(kafkaTopic,bytes);
        if (BATCH_KAFKA_MESSAGES) {
          messagesToWrite.add(data);
        }
 else {
          producer.send(data);
        }
        recordCount+=1;
      }
      if (BATCH_KAFKA_MESSAGES) {
        producer.send(messagesToWrite);
      }
      outputStream.close();
      reader.close();
      LOGGER.info("Finished writing " + recordCount + " records from "+ avroFile.getName()+ " into Kafka topic "+ kafkaTopic);
      int totalRecordCount=totalAvroRecordWrittenCount.addAndGet(recordCount);
      LOGGER.info("Total records written so far " + totalRecordCount);
    }
 catch (    Exception e) {
      e.printStackTrace();
      throw new RuntimeException(e);
    }
  }
}
