{
  Job job=Job.getInstance(getConf());
  job.setJarByClass(SegmentCreationPhaseJob.class);
  job.setJobName(name);
  FileSystem fs=FileSystem.get(getConf());
  Configuration configuration=job.getConfiguration();
  String schemaPath=getAndSetConfiguration(configuration,SEGMENT_CREATION_SCHEMA_PATH);
  LOGGER.info("Schema path : {}",schemaPath);
  String configPath=getAndSetConfiguration(configuration,SEGMENT_CREATION_CONFIG_PATH);
  LOGGER.info("Config path : {}",configPath);
  StarTreeConfig starTreeConfig=StarTreeConfig.decode(fs.open(new Path(configPath)));
  Schema dataSchema=ThirdeyePinotSchemaUtils.createSchema(configPath);
  LOGGER.info("Data schema : {}",dataSchema);
  String inputSegmentDir=getAndSetConfiguration(configuration,SEGMENT_CREATION_INPUT_PATH);
  LOGGER.info("Input path : {}",inputSegmentDir);
  String outputDir=getAndSetConfiguration(configuration,SEGMENT_CREATION_OUTPUT_PATH);
  LOGGER.info("Output path : {}",outputDir);
  String stagingDir=new File(outputDir,TEMP).getAbsolutePath();
  LOGGER.info("Staging dir : {}",stagingDir);
  String tableName=getAndSetConfiguration(configuration,SEGMENT_CREATION_SEGMENT_TABLE_NAME);
  LOGGER.info("Segment table name : {}",tableName);
  if (fs.exists(new Path(stagingDir))) {
    LOGGER.warn("Found the temp folder, deleting it");
    fs.delete(new Path(stagingDir),true);
  }
  fs.mkdirs(new Path(stagingDir));
  fs.mkdirs(new Path(stagingDir + "/input/"));
  if (fs.exists(new Path(outputDir))) {
    LOGGER.warn("Found the output folder deleting it");
    fs.delete(new Path(outputDir),true);
  }
  fs.mkdirs(new Path(outputDir));
  Path inputPathPattern=new Path(inputSegmentDir);
  List<FileStatus> inputDataFiles=Arrays.asList(fs.listStatus(inputPathPattern));
  LOGGER.info("size {}",inputDataFiles.size());
  try {
    for (int seqId=0; seqId < inputDataFiles.size(); ++seqId) {
      FileStatus file=inputDataFiles.get(seqId);
      String completeFilePath=" " + file.getPath().toString() + " "+ seqId;
      Path newOutPutFile=new Path((stagingDir + "/input/" + file.getPath().toString().replace('.','_').replace('/','_').replace(':','_')+ ".txt"));
      FSDataOutputStream stream=fs.create(newOutPutFile);
      LOGGER.info("wrote {}",completeFilePath);
      stream.writeUTF(completeFilePath);
      stream.flush();
      stream.close();
    }
  }
 catch (  Exception e) {
    LOGGER.error("Exception while reading input files ",e);
  }
  job.setMapperClass(SegmentCreationPhaseMapReduceJob.SegmentCreationMapper.class);
  if (System.getenv("HADOOP_TOKEN_FILE_LOCATION") != null) {
    job.getConfiguration().set("mapreduce.job.credentials.binary",System.getenv("HADOOP_TOKEN_FILE_LOCATION"));
  }
  job.setInputFormatClass(TextInputFormat.class);
  job.setOutputFormatClass(TextOutputFormat.class);
  job.setMapOutputKeyClass(LongWritable.class);
  job.setMapOutputValueClass(Text.class);
  FileInputFormat.addInputPath(job,new Path(stagingDir + "/input/"));
  FileOutputFormat.setOutputPath(job,new Path(stagingDir + "/output/"));
  job.getConfiguration().setInt(JobContext.NUM_MAPS,inputDataFiles.size());
  job.getConfiguration().set(SEGMENT_CREATION_DATA_SCHEMA.toString(),OBJECT_MAPPER.writeValueAsString(dataSchema));
  if (!fs.exists(new Path(schemaPath))) {
    OBJECT_MAPPER.writerWithDefaultPrettyPrinter().writeValue(fs.create(new Path(schemaPath),false),dataSchema);
  }
  job.getConfiguration().set(SEGMENT_CREATION_STARTREE_CONFIG.toString(),OBJECT_MAPPER.writeValueAsString(starTreeConfig));
  job.setMaxReduceAttempts(1);
  job.setMaxMapAttempts(0);
  job.setNumReduceTasks(0);
  for (  Object key : props.keySet()) {
    job.getConfiguration().set(key.toString(),props.getProperty(key.toString()));
  }
  job.waitForCompletion(true);
  if (!job.isSuccessful()) {
    throw new RuntimeException("Job failed : " + job);
  }
  LOGGER.info("Moving Segment Tar files from {} to: {}",stagingDir + "/output/segmentTar",outputDir);
  FileStatus[] segmentArr=fs.listStatus(new Path(stagingDir + "/output/segmentTar"));
  for (  FileStatus segment : segmentArr) {
    fs.rename(segment.getPath(),new Path(outputDir,segment.getPath().getName()));
  }
  LOGGER.info("Cleanup the working directory.");
  LOGGER.info("Deleting the dir: {}",stagingDir);
  fs.delete(new Path(stagingDir),true);
  return job;
}
