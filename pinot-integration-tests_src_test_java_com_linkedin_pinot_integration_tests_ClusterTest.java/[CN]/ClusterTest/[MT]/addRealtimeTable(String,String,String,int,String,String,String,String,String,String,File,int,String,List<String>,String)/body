{
  JSONObject streamConfig=new JSONObject();
  streamConfig.put("streamType","kafka");
  streamConfig.put(DataSource.STREAM_PREFIX + "." + Kafka.CONSUMER_TYPE,Kafka.ConsumerType.highLevel.toString());
  streamConfig.put(DataSource.STREAM_PREFIX + "." + Kafka.TOPIC_NAME,kafkaTopic);
  streamConfig.put(DataSource.STREAM_PREFIX + "." + Kafka.DECODER_CLASS,AvroFileSchemaKafkaAvroMessageDecoder.class.getName());
  streamConfig.put(DataSource.STREAM_PREFIX + "." + Kafka.ZK_BROKER_URL,kafkaZkUrl);
  streamConfig.put(DataSource.STREAM_PREFIX + "." + Kafka.HighLevelConsumer.ZK_CONNECTION_STRING,kafkaZkUrl);
  streamConfig.put(DataSource.Realtime.REALTIME_SEGMENT_FLUSH_SIZE,Integer.toString(realtimeSegmentFlushSize));
  streamConfig.put(DataSource.STREAM_PREFIX + "." + Kafka.KAFKA_CONSUMER_PROPS_PREFIX+ "."+ "auto.offset.reset","smallest");
  JSONObject request=ControllerRequestBuilder.buildCreateRealtimeTableJSON(tableName,serverTenant,brokerTenant,timeColumnName,timeColumnType,retentionTimeUnit,Integer.toString(retentionDays),1,"BalanceNumSegmentAssignmentStrategy",streamConfig,schemaName,sortedColumn,invertedIndexColumns,null,true);
  sendPostRequest(ControllerRequestURLBuilder.baseUrl(CONTROLLER_BASE_API_URL).forTableCreate(),request.toString());
  AvroFileSchemaKafkaAvroMessageDecoder.avroFile=avroFile;
}
