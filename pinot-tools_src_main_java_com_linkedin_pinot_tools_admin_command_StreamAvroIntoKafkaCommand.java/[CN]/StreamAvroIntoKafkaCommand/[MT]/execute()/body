{
  int messageDelayMillis=Integer.parseInt(_millisBetweenMessages);
  final boolean sleepRequired=0 < messageDelayMillis;
  if (sleepRequired) {
    LOGGER.info("Streaming Avro file into Kafka topic {} with {} ms between messages",_kafkaTopic,_millisBetweenMessages);
  }
 else {
    LOGGER.info("Streaming Avro file into Kafka topic {} with no delay between messages",_kafkaTopic);
  }
  Properties properties=new Properties();
  properties.put("metadata.broker.list",_kafkaBrokerList);
  properties.put("serializer.class","kafka.serializer.DefaultEncoder");
  properties.put("request.required.acks","1");
  ProducerConfig producerConfig=new ProducerConfig(properties);
  Producer<byte[],byte[]> producer=new Producer<byte[],byte[]>(producerConfig);
  try {
    DataFileStream<GenericRecord> reader=AvroUtils.getAvroReader(new File(_avroFile));
    for (    GenericRecord genericRecord : reader) {
      String recordJson=genericRecord.toString();
      byte[] bytes=recordJson.getBytes("utf-8");
      KeyedMessage<byte[],byte[]> data=new KeyedMessage<byte[],byte[]>(_kafkaTopic,Longs.toByteArray(HashUtil.hash64(bytes,bytes.length)),bytes);
      producer.send(data);
      if (sleepRequired) {
        Uninterruptibles.sleepUninterruptibly(messageDelayMillis,TimeUnit.MILLISECONDS);
      }
    }
    reader.close();
  }
 catch (  Exception e) {
    e.printStackTrace();
    throw new RuntimeException(e);
  }
  savePID(System.getProperty("java.io.tmpdir") + File.separator + ".streamAvro.pid");
  return true;
}
